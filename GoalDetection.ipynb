{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Flower_Classification_with_TFLite_Model_Maker (Beta).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MannoMation/SkystoneSimulator/blob/master/GoalDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Flower classification with TensorFlow Lite Model Maker with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDABAblytltI"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/codelabs/flower_classification/ml/Flower_Classification_with_TFLite_Model_Maker.ipynb\">      \n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/ml/Flower_Classification_with_TFLite_Model_Maker.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86-Nh4pMHqY"
      },
      "source": [
        "Model Maker library simplifies the process of adapting and converting a TensorFlow neural-network model to particular input data when deploying this model for on-device ML applications.\n",
        "\n",
        "This notebook shows an end-to-end example that utilizes this Model Maker library to illustrate the adaption and conversion of a commonly-used image classification model to classify flowers on a mobile device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run this example, we first need to make a copy of this notebook. Click on \"Copy to Drive\" at the top of this notebook. Then we need to install serveral required packages, including Model Maker package that in github [repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cv3K3oaksJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3caff8-1e72-4548-9a3e-165fcaf78b08"
      },
      "source": [
        "!pip install -q tflite-model-maker"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 593kB 27.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3MB 48.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 58.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 62.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 849kB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 42.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 55.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.2MB 69kB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 47.4MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag6XgUy8TYZR"
      },
      "source": [
        "pip install -q pycocotools"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx1HGRoFQ54j"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b60165e-c1a7-4c6f-b2a3-13b1b35f3bd1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.image_classifier import DataLoader\n",
        "from tflite_model_maker import object_detector\n",
        "from tflite_model_maker import model_spec\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKRaYHABpob5"
      },
      "source": [
        "## Simple End-to-End Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFOLYJuo5IQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2e036d-a0be-4645-f143-1578ac29d00a"
      },
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\"/gdrive/MyDrive/Goal Detection/TrainingData\", \"/gdrive/MyDrive/Goal Detection/TrainingData\", label_map={1: \"Goal\"})\n",
        "print(len(train_data))\n",
        "#validation_data = object_detector.DataLoader.from_pascal_voc(\"/gdrive/MyDrive/Goal Detection/TrainingData/\", \"/gdrive/MyDrive/Goal Detection/TrainingData/\", label_map={1: \"Goal\"})\n",
        "#test_data = object_detector.DataLoader.from_pascal_voc(\"/gdrive/MyDrive/Goal Detection/TrainingData/\", \"/gdrive/MyDrive/Goal Detection/TrainingData/\", label_map={1: \"Goal\"})\n",
        "\n",
        "#train_data, test_data = dataloader.split(0.9)\n",
        "#train_data = dataloader.gen_dataset\n",
        "#test_data = dataloader.gen_dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Cache will be stored in /tmp/tmpg_g1nlk3 with prefix filename cf0923bf2349a985988c77584edda788. Cache_prefix is /tmp/tmpg_g1nlk3/cf0923bf2349a985988c77584edda788\n",
            "INFO:tensorflow:On image 0\n",
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWmSHdBr7oBT"
      },
      "source": [
        "validation_data = train_data\n",
        "test_data = train_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiZZ5DHXotaW"
      },
      "source": [
        "### Get the data path\n",
        "\n",
        "Let's get some images to play with this simple end-to-end example. Hundreds of images is a good start for Model Maker while more data could achieve better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "3jz5x0JoskPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0560d89a-a2a5-44bb-e27f-4fdddcea6f86"
      },
      "source": [
        "image_path = tf.keras.utils.get_file(\n",
        "      'flower_photos',\n",
        "      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "      untar=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55MR6i6nuDm"
      },
      "source": [
        "You could replace `image_path` with your own image folders. As for uploading data to colab, you could find the upload button in the left sidebar shown in the image below with the red rectangle. Just have a try to upload a zip file and unzip it. The root file path is the current path.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_image_classification.png\" alt=\"Upload File\" width=\"800\" hspace=\"100\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRNv_mloS89"
      },
      "source": [
        "If you prefer not to upload your images to the cloud, you could try to run the library locally following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) in github."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-VDriAdsowu"
      },
      "source": [
        "### Run the example\n",
        "The example just consists of 4 lines of code as shown below, each of which representing one step of the overall process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ahtcO86tZBL"
      },
      "source": [
        "1.   Load input data specific to an on-device ML app. Split it to training data and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lANoNS_gtdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7a4951-bd85-427b-e522-d7a16662aece"
      },
      "source": [
        "data = DataLoader.from_folder(image_path)\n",
        "train_data, test_data = data.split(0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Load image with size: 3670, num_label: 5, labels: daisy, dandelion, roses, sunflowers, tulips.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_9IWyIztuRF"
      },
      "source": [
        "2. Customize the TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRXMZbrwtyRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0268bec2-d551-46cc-befe-04192aadde4f"
      },
      "source": [
        "#model = image_classifier.create(train_data)\n",
        "spec = model_spec.get('efficientdet_lite0')\n",
        "model = object_detector.create(train_data, model_spec=spec, epochs=50, batch_size=8, train_whole_model=True, validation_data=validation_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 6/10 [=================>............] - ETA: 1s - det_loss: 1.7106 - cls_loss: 1.1382 - box_loss: 0.0114 - reg_l2_loss: 0.0630 - loss: 1.7736 - learning_rate: 0.0085 - gradient_norm: 2.1092WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1911s vs `on_train_batch_end` time: 0.2355s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1911s vs `on_train_batch_end` time: 0.2355s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 33s 701ms/step - det_loss: 1.6640 - cls_loss: 1.1070 - box_loss: 0.0111 - reg_l2_loss: 0.0630 - loss: 1.7270 - learning_rate: 0.0090 - gradient_norm: 1.9721 - val_det_loss: 1.5032 - val_cls_loss: 1.0304 - val_box_loss: 0.0095 - val_reg_l2_loss: 0.0630 - val_loss: 1.5662\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 3s 315ms/step - det_loss: 1.2850 - cls_loss: 0.8146 - box_loss: 0.0094 - reg_l2_loss: 0.0630 - loss: 1.3480 - learning_rate: 0.0100 - gradient_norm: 2.1927 - val_det_loss: 1.6472 - val_cls_loss: 1.2011 - val_box_loss: 0.0089 - val_reg_l2_loss: 0.0630 - val_loss: 1.7102\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 3s 314ms/step - det_loss: 0.8739 - cls_loss: 0.5021 - box_loss: 0.0074 - reg_l2_loss: 0.0630 - loss: 0.9369 - learning_rate: 0.0099 - gradient_norm: 2.9497 - val_det_loss: 0.9252 - val_cls_loss: 0.5417 - val_box_loss: 0.0077 - val_reg_l2_loss: 0.0630 - val_loss: 0.9882\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 3s 316ms/step - det_loss: 0.6754 - cls_loss: 0.3552 - box_loss: 0.0064 - reg_l2_loss: 0.0630 - loss: 0.7384 - learning_rate: 0.0099 - gradient_norm: 2.4040 - val_det_loss: 0.7062 - val_cls_loss: 0.4105 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0630 - val_loss: 0.7692\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 3s 317ms/step - det_loss: 0.5105 - cls_loss: 0.2687 - box_loss: 0.0048 - reg_l2_loss: 0.0630 - loss: 0.5735 - learning_rate: 0.0098 - gradient_norm: 1.9841 - val_det_loss: 0.6196 - val_cls_loss: 0.3073 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0630 - val_loss: 0.6826\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.40s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.853\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.191\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.298\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.230\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.267\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.477\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 3s 314ms/step - det_loss: 0.4974 - cls_loss: 0.2682 - box_loss: 0.0046 - reg_l2_loss: 0.0630 - loss: 0.5604 - learning_rate: 0.0097 - gradient_norm: 1.9693 - val_det_loss: 0.4511 - val_cls_loss: 0.2208 - val_box_loss: 0.0046 - val_reg_l2_loss: 0.0630 - val_loss: 0.5141\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 3s 317ms/step - det_loss: 0.3879 - cls_loss: 0.2132 - box_loss: 0.0035 - reg_l2_loss: 0.0630 - loss: 0.4509 - learning_rate: 0.0096 - gradient_norm: 1.8807 - val_det_loss: 0.5171 - val_cls_loss: 0.2936 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.0631 - val_loss: 0.5802\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 3s 317ms/step - det_loss: 0.3731 - cls_loss: 0.2060 - box_loss: 0.0033 - reg_l2_loss: 0.0631 - loss: 0.4361 - learning_rate: 0.0094 - gradient_norm: 1.8281 - val_det_loss: 0.5089 - val_cls_loss: 0.2618 - val_box_loss: 0.0049 - val_reg_l2_loss: 0.0631 - val_loss: 0.5719\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 3s 315ms/step - det_loss: 0.3213 - cls_loss: 0.1892 - box_loss: 0.0026 - reg_l2_loss: 0.0631 - loss: 0.3844 - learning_rate: 0.0093 - gradient_norm: 1.9430 - val_det_loss: 0.3399 - val_cls_loss: 0.1875 - val_box_loss: 0.0030 - val_reg_l2_loss: 0.0631 - val_loss: 0.4030\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 3s 319ms/step - det_loss: 0.3026 - cls_loss: 0.1781 - box_loss: 0.0025 - reg_l2_loss: 0.0631 - loss: 0.3657 - learning_rate: 0.0091 - gradient_norm: 1.8336 - val_det_loss: 0.3069 - val_cls_loss: 0.1869 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0631 - val_loss: 0.3699\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.851\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.701\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.699\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.752\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.700\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.761\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 3s 327ms/step - det_loss: 0.2617 - cls_loss: 0.1611 - box_loss: 0.0020 - reg_l2_loss: 0.0631 - loss: 0.3248 - learning_rate: 0.0089 - gradient_norm: 1.6825 - val_det_loss: 0.2838 - val_cls_loss: 0.1509 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.3468\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 3s 326ms/step - det_loss: 0.2782 - cls_loss: 0.1617 - box_loss: 0.0023 - reg_l2_loss: 0.0631 - loss: 0.3413 - learning_rate: 0.0087 - gradient_norm: 1.5927 - val_det_loss: 0.2985 - val_cls_loss: 0.1518 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0631 - val_loss: 0.3616\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 3s 330ms/step - det_loss: 0.2655 - cls_loss: 0.1546 - box_loss: 0.0022 - reg_l2_loss: 0.0631 - loss: 0.3286 - learning_rate: 0.0085 - gradient_norm: 1.6902 - val_det_loss: 0.2094 - val_cls_loss: 0.1312 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0631 - val_loss: 0.2725\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 3s 333ms/step - det_loss: 0.2586 - cls_loss: 0.1485 - box_loss: 0.0022 - reg_l2_loss: 0.0631 - loss: 0.3217 - learning_rate: 0.0082 - gradient_norm: 1.8805 - val_det_loss: 0.1856 - val_cls_loss: 0.1271 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0631 - val_loss: 0.2487\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 3s 330ms/step - det_loss: 0.2227 - cls_loss: 0.1369 - box_loss: 0.0017 - reg_l2_loss: 0.0631 - loss: 0.2858 - learning_rate: 0.0080 - gradient_norm: 1.3841 - val_det_loss: 0.1834 - val_cls_loss: 0.1228 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0631 - val_loss: 0.2464\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.29s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.699\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.985\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.930\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.713\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.704\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.787\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.787\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.633\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 3s 323ms/step - det_loss: 0.2301 - cls_loss: 0.1344 - box_loss: 0.0019 - reg_l2_loss: 0.0631 - loss: 0.2932 - learning_rate: 0.0077 - gradient_norm: 1.5561 - val_det_loss: 0.1997 - val_cls_loss: 0.1140 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0631 - val_loss: 0.2628\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 3s 321ms/step - det_loss: 0.2328 - cls_loss: 0.1264 - box_loss: 0.0021 - reg_l2_loss: 0.0631 - loss: 0.2959 - learning_rate: 0.0075 - gradient_norm: 1.5742 - val_det_loss: 0.1722 - val_cls_loss: 0.1107 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0631 - val_loss: 0.2353\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 3s 311ms/step - det_loss: 0.2299 - cls_loss: 0.1317 - box_loss: 0.0020 - reg_l2_loss: 0.0631 - loss: 0.2930 - learning_rate: 0.0072 - gradient_norm: 1.4522 - val_det_loss: 0.1639 - val_cls_loss: 0.0932 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0631 - val_loss: 0.2270\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 3s 318ms/step - det_loss: 0.1883 - cls_loss: 0.1129 - box_loss: 0.0015 - reg_l2_loss: 0.0631 - loss: 0.2514 - learning_rate: 0.0069 - gradient_norm: 1.3896 - val_det_loss: 0.1469 - val_cls_loss: 0.0946 - val_box_loss: 0.0010 - val_reg_l2_loss: 0.0631 - val_loss: 0.2100\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 3s 310ms/step - det_loss: 0.1797 - cls_loss: 0.1130 - box_loss: 0.0013 - reg_l2_loss: 0.0631 - loss: 0.2428 - learning_rate: 0.0066 - gradient_norm: 1.2832 - val_det_loss: 0.1618 - val_cls_loss: 0.1042 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0631 - val_loss: 0.2249\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.30s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.731\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.999\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.999\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.817\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.725\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.731\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.845\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.845\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.845\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 3s 313ms/step - det_loss: 0.1704 - cls_loss: 0.1084 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2335 - learning_rate: 0.0063 - gradient_norm: 1.1862 - val_det_loss: 0.1349 - val_cls_loss: 0.0915 - val_box_loss: 8.6834e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1980\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 3s 315ms/step - det_loss: 0.1650 - cls_loss: 0.1038 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2281 - learning_rate: 0.0060 - gradient_norm: 1.4547 - val_det_loss: 0.1848 - val_cls_loss: 0.1241 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0631 - val_loss: 0.2479\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 3s 322ms/step - det_loss: 0.1906 - cls_loss: 0.1211 - box_loss: 0.0014 - reg_l2_loss: 0.0631 - loss: 0.2537 - learning_rate: 0.0056 - gradient_norm: 1.7288 - val_det_loss: 0.1209 - val_cls_loss: 0.0901 - val_box_loss: 6.1548e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1840\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 3s 321ms/step - det_loss: 0.1785 - cls_loss: 0.1061 - box_loss: 0.0014 - reg_l2_loss: 0.0631 - loss: 0.2416 - learning_rate: 0.0053 - gradient_norm: 1.3051 - val_det_loss: 0.1237 - val_cls_loss: 0.0897 - val_box_loss: 6.7974e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1868\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 3s 325ms/step - det_loss: 0.1616 - cls_loss: 0.1025 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2247 - learning_rate: 0.0050 - gradient_norm: 1.2616 - val_det_loss: 0.1148 - val_cls_loss: 0.0849 - val_box_loss: 5.9836e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1779\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.835\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.970\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.764\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.849\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.879\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.880\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.886\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 4s 392ms/step - det_loss: 0.1464 - cls_loss: 0.1001 - box_loss: 9.2652e-04 - reg_l2_loss: 0.0631 - loss: 0.2095 - learning_rate: 0.0047 - gradient_norm: 1.2179 - val_det_loss: 0.1261 - val_cls_loss: 0.0853 - val_box_loss: 8.1525e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1892\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 3s 334ms/step - det_loss: 0.1798 - cls_loss: 0.1088 - box_loss: 0.0014 - reg_l2_loss: 0.0631 - loss: 0.2429 - learning_rate: 0.0044 - gradient_norm: 1.3452 - val_det_loss: 0.1127 - val_cls_loss: 0.0756 - val_box_loss: 7.4314e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1758\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 3s 325ms/step - det_loss: 0.1605 - cls_loss: 0.1016 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2236 - learning_rate: 0.0040 - gradient_norm: 1.3302 - val_det_loss: 0.1496 - val_cls_loss: 0.0818 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0631 - val_loss: 0.2127\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 3s 336ms/step - det_loss: 0.1528 - cls_loss: 0.0932 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2158 - learning_rate: 0.0037 - gradient_norm: 1.3326 - val_det_loss: 0.1127 - val_cls_loss: 0.0766 - val_box_loss: 7.2249e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1758\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 3s 326ms/step - det_loss: 0.1356 - cls_loss: 0.0903 - box_loss: 9.0602e-04 - reg_l2_loss: 0.0631 - loss: 0.1987 - learning_rate: 0.0034 - gradient_norm: 1.0499 - val_det_loss: 0.1139 - val_cls_loss: 0.0808 - val_box_loss: 6.6183e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1770\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.836\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.886\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.832\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.854\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.890\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.891\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.891\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 3s 317ms/step - det_loss: 0.1460 - cls_loss: 0.0949 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2091 - learning_rate: 0.0031 - gradient_norm: 1.2346 - val_det_loss: 0.1182 - val_cls_loss: 0.0784 - val_box_loss: 7.9682e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1813\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 3s 328ms/step - det_loss: 0.1487 - cls_loss: 0.0952 - box_loss: 0.0011 - reg_l2_loss: 0.0631 - loss: 0.2117 - learning_rate: 0.0028 - gradient_norm: 1.2401 - val_det_loss: 0.1171 - val_cls_loss: 0.0743 - val_box_loss: 8.5600e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1802\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 3s 331ms/step - det_loss: 0.1499 - cls_loss: 0.0910 - box_loss: 0.0012 - reg_l2_loss: 0.0631 - loss: 0.2129 - learning_rate: 0.0025 - gradient_norm: 1.1570 - val_det_loss: 0.0917 - val_cls_loss: 0.0695 - val_box_loss: 4.4435e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1548\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 3s 327ms/step - det_loss: 0.1395 - cls_loss: 0.0914 - box_loss: 9.6152e-04 - reg_l2_loss: 0.0631 - loss: 0.2026 - learning_rate: 0.0023 - gradient_norm: 1.3138 - val_det_loss: 0.1009 - val_cls_loss: 0.0744 - val_box_loss: 5.2919e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1640\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 3s 326ms/step - det_loss: 0.1399 - cls_loss: 0.0906 - box_loss: 9.8573e-04 - reg_l2_loss: 0.0631 - loss: 0.2030 - learning_rate: 0.0020 - gradient_norm: 1.0518 - val_det_loss: 0.0974 - val_cls_loss: 0.0732 - val_box_loss: 4.8332e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1604\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.853\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.871\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.853\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.851\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.933\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.897\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 3s 332ms/step - det_loss: 0.1409 - cls_loss: 0.0907 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2039 - learning_rate: 0.0018 - gradient_norm: 1.2204 - val_det_loss: 0.0968 - val_cls_loss: 0.0706 - val_box_loss: 5.2458e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1599\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 3s 333ms/step - det_loss: 0.1403 - cls_loss: 0.0896 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2034 - learning_rate: 0.0015 - gradient_norm: 1.1315 - val_det_loss: 0.0927 - val_cls_loss: 0.0684 - val_box_loss: 4.8708e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1558\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 3s 321ms/step - det_loss: 0.1492 - cls_loss: 0.1001 - box_loss: 9.8299e-04 - reg_l2_loss: 0.0631 - loss: 0.2123 - learning_rate: 0.0013 - gradient_norm: 1.2583 - val_det_loss: 0.0904 - val_cls_loss: 0.0673 - val_box_loss: 4.6240e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1535\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 3s 323ms/step - det_loss: 0.1420 - cls_loss: 0.0905 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2051 - learning_rate: 0.0011 - gradient_norm: 1.0214 - val_det_loss: 0.0985 - val_cls_loss: 0.0693 - val_box_loss: 5.8324e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1616\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 3s 324ms/step - det_loss: 0.1434 - cls_loss: 0.0912 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2064 - learning_rate: 9.0158e-04 - gradient_norm: 1.1268 - val_det_loss: 0.0982 - val_cls_loss: 0.0691 - val_box_loss: 5.8079e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1612\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.856\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.886\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.853\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.866\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.903\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.904\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.904\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 3s 325ms/step - det_loss: 0.1499 - cls_loss: 0.0948 - box_loss: 0.0011 - reg_l2_loss: 0.0631 - loss: 0.2130 - learning_rate: 7.2660e-04 - gradient_norm: 1.1007 - val_det_loss: 0.0913 - val_cls_loss: 0.0695 - val_box_loss: 4.3695e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1544\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 3s 320ms/step - det_loss: 0.1263 - cls_loss: 0.0822 - box_loss: 8.8192e-04 - reg_l2_loss: 0.0631 - loss: 0.1893 - learning_rate: 5.6919e-04 - gradient_norm: 0.9321 - val_det_loss: 0.0912 - val_cls_loss: 0.0699 - val_box_loss: 4.2476e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1542\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 3s 315ms/step - det_loss: 0.1317 - cls_loss: 0.0860 - box_loss: 9.1531e-04 - reg_l2_loss: 0.0631 - loss: 0.1948 - learning_rate: 4.2998e-04 - gradient_norm: 1.0272 - val_det_loss: 0.0903 - val_cls_loss: 0.0699 - val_box_loss: 4.0928e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1534\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 3s 324ms/step - det_loss: 0.1285 - cls_loss: 0.0842 - box_loss: 8.8524e-04 - reg_l2_loss: 0.0631 - loss: 0.1915 - learning_rate: 3.0955e-04 - gradient_norm: 1.0009 - val_det_loss: 0.0909 - val_cls_loss: 0.0696 - val_box_loss: 4.2701e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1540\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 3s 314ms/step - det_loss: 0.1435 - cls_loss: 0.0920 - box_loss: 0.0010 - reg_l2_loss: 0.0631 - loss: 0.2065 - learning_rate: 2.0839e-04 - gradient_norm: 1.2325 - val_det_loss: 0.0913 - val_cls_loss: 0.0692 - val_box_loss: 4.4122e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1544\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.876\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.903\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.881\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.904\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.904\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.917\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.903\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 3s 319ms/step - det_loss: 0.1318 - cls_loss: 0.0875 - box_loss: 8.8679e-04 - reg_l2_loss: 0.0631 - loss: 0.1949 - learning_rate: 1.2693e-04 - gradient_norm: 1.0956 - val_det_loss: 0.0898 - val_cls_loss: 0.0688 - val_box_loss: 4.2095e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1529\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 3s 329ms/step - det_loss: 0.1282 - cls_loss: 0.0846 - box_loss: 8.7279e-04 - reg_l2_loss: 0.0631 - loss: 0.1913 - learning_rate: 6.5487e-05 - gradient_norm: 1.0765 - val_det_loss: 0.0888 - val_cls_loss: 0.0684 - val_box_loss: 4.0734e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1518\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 3s 336ms/step - det_loss: 0.1427 - cls_loss: 0.0901 - box_loss: 0.0011 - reg_l2_loss: 0.0631 - loss: 0.2058 - learning_rate: 2.4323e-05 - gradient_norm: 1.0630 - val_det_loss: 0.0885 - val_cls_loss: 0.0683 - val_box_loss: 4.0441e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1516\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 3s 332ms/step - det_loss: 0.1219 - cls_loss: 0.0829 - box_loss: 7.7942e-04 - reg_l2_loss: 0.0631 - loss: 0.1850 - learning_rate: 3.6053e-06 - gradient_norm: 1.0304 - val_det_loss: 0.0886 - val_cls_loss: 0.0684 - val_box_loss: 4.0480e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1517\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 3s 329ms/step - det_loss: 0.1375 - cls_loss: 0.0895 - box_loss: 9.5902e-04 - reg_l2_loss: 0.0631 - loss: 0.2005 - learning_rate: 3.4186e-06 - gradient_norm: 1.0288 - val_det_loss: 0.0887 - val_cls_loss: 0.0686 - val_box_loss: 4.0367e-04 - val_reg_l2_loss: 0.0631 - val_loss: 0.1518\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8000, 7)\n",
            "0/8000\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.36s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.877\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.886\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.877\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.885\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.909\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.909\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxU2fDr-t2Ya"
      },
      "source": [
        "3. Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQr02VxJt6Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b155f86-e392-4f8c-9296-bead3e909083"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function EfficientDetModelSpec.evaluate.<locals>._get_detections at 0x7f4895b7af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function EfficientDetModelSpec.evaluate.<locals>._get_detections at 0x7f4895b7af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1/2 [==============>...............] - ETA: 6sWARNING:tensorflow:6 out of the last 6 calls to <function EfficientDetModelSpec.evaluate.<locals>._get_detections at 0x7f4895b7af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function EfficientDetModelSpec.evaluate.<locals>._get_detections at 0x7f4895b7af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 9s 3s/step\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(8300, 7)\n",
            "0/8300\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.878\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.886\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.878\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.888\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.911\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.911\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.8778713,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 1.0,\n",
              " 'AP_/Goal': 0.8778713,\n",
              " 'APl': 0.8777109,\n",
              " 'APm': 0.8859736,\n",
              " 'APs': -1.0,\n",
              " 'ARl': 0.9116883,\n",
              " 'ARm': 0.9,\n",
              " 'ARmax1': 0.8879518,\n",
              " 'ARmax10': 0.9108434,\n",
              " 'ARmax100': 0.9108434,\n",
              " 'ARs': -1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZw9zU8t84y"
      },
      "source": [
        "4.  Export to TensorFlow Lite model.\n",
        "You could download it in the left sidebar same as the uploading part for your own use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-eIzfluCoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "d410957e-f45a-4724-aaff-33c9d313188c"
      },
      "source": [
        "#model.export(export_dir='.')\n",
        "model.export(export_dir='.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5359c672a191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.export(export_dir='.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/custom_model.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, export_dir, tflite_filename, label_filename, vocab_filename, saved_model_filename, tfjs_folder_name, export_format, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mtflite_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtflite_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mexport_tflite_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_tflite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexport_tflite_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m       tf.compat.v1.logging.info(\n\u001b[1;32m    164\u001b[0m           'TensorFlow Lite model exported successfully: %s' % tflite_filepath)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py\u001b[0m in \u001b[0;36m_export_tflite\u001b[0;34m(self, tflite_filepath, quantization_config, with_metadata, export_metadata_json_file)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     self.model_spec.export_tflite(self.model, tflite_filepath,\n\u001b[0;32m--> 219\u001b[0;31m                                   quantization_config)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py\u001b[0m in \u001b[0;36mexport_tflite\u001b[0;34m(self, model, tflite_filepath, quantization_config)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                            \"File isn't open for writing\")\n\u001b[1;32m     87\u001b[0m       self._writable_file = _pywrap_file_io.WritableFile(\n\u001b[0;32m---> 88\u001b[0;31m           compat.path_to_bytes(self.__name), compat.as_bytes(self.__mode))\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: ./model.tflite; Operation not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyju1qc_v-wy"
      },
      "source": [
        "5. Download the trained model by clicking on the folder icon on the left hand side. Right-click on \"model.tflite\" and select download. Or run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hSmJsgWM0Lp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0629c0e3-b5b3-4730-88e0-c6d8b7e78620"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.tflite') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_470d3b4a-442c-44bc-964c-b437b321fea3\", \"model.tflite\", 4013541)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QujcbgibNR1e"
      },
      "source": [
        "After this simple 5 steps, we can now continue to the next step in the [codelab](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android-beta/#2).\n",
        "\n",
        "For a more comprehensive guide to TFLite Model Maker, please refer to this [notebook](https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb) and its [documentation](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    }
  ]
}